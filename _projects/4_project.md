---
layout: page
title: Ethical Risk for Countering the Financing of Terrorism Algorithms
description: CFT algorithms enhance financial crime detection but require inclusive, bias-aware strategies to prevent harm to vulnerable populations and ensure equitable access to financial systems.
img: assets/img/watermelon.jpg
importance: 1
category: ethics & algorithms
---

## Summary
Countering the Financing of Terrorism (CFT) algorithms, utilized by governments and financial institutions, aim to enhance the detection of illicit financial activities but risk exacerbating systemic inequities. These algorithms can disproportionately impact marginalized groups, exclude vulnerable populations, and perpetuate biases due to reliance on incomplete or biased data. To ensure ethical implementation, the lifecycle of these AI systems must prioritize fairness, inclusivity, and transparency through robust governance, adaptive inclusion strategies, and fairness-aware algorithmic design. A commitment to equity in CFT systems will enable the balancing of security imperatives with the protection of civil liberties and financial access.

## Overview of Algorithmic System 
The United States Treasury partners with key government agencies and private financial institutions to strengthen efforts in [Countering the Financing of Terrorism](https://home.treasury.gov/about/offices/terrorism-and-financial-intelligence/office-of-terrorist-financing-and-financial-crimes) (CFT) and promotes the [adoption of artificial intelligence systems](https://home.treasury.gov/system/files/136/Artificial-Intelligence-in-Financial-Services.pdf) to enhance capabilities. Additionally, they are committed to implementing standards set by the [Financial Action Task Force](https://home.treasury.gov/policy-issues/terrorism-and-illicit-finance/financial-action-task-force) (FATF), an international body that promotes financial system safeguarding policies, including [recommendations on AI use](https://www.fatf-gafi.org/en/publications/Digitaltransformation/Digital-transformation.html) in CFT. 
 
Such AI models [detect behavioral patterns](https://www2.deloitte.com/content/dam/Deloitte/jp/Documents/financial-services/bk/en-the-case-for-artificial-intelligence-in-combating-money-laundering-and-terrorist-financing.pdf) that may be correlated with fraudulent or illegal activity by processing large amounts of data regarding the business portfolio and operations, financial crime typologies, and transactional data. Algorithms used by these models may also incorporate aspects of [Perpetual Know Your Customer](https://www.acams.org/en/best-practice-guide-the-path-to-perpetual-kyc#benefits-783e4ff5) (pKYC) or [Perpetual Client Risk Assessment](https://fintech.global/2024/09/23/how-fincens-new-aml-cft-proposal-leverages-ai-for-better-compliance/) (pCRA), processes which dynamically assess client profiles and risk. In response to data privacy concerns, some AI financial tools are testing the use of [federated learning](https://developer.nvidia.com/blog/using-federated-learning-to-bridge-data-silos-in-financial-services/), which allows for the model to be trained on decentralized personal data. 

CFT algorithms detect anomalous activity patterns and then flag them for review by a human-in-the-loop who must contextualize and interpret the output. [Escalations](https://www2.deloitte.com/content/dam/Deloitte/jp/Documents/financial-services/bk/en-the-case-for-artificial-intelligence-in-combating-money-laundering-and-terrorist-financing.pdf) may include investigation, restricted access to or termination of financial services, and notification of relevant authorities. 
The stakeholders of this system encompass a diverse range of entities, from the banking institution itself and its shareholders to government agencies like the US Treasury and [FinCEN](https://www.federalregister.gov/documents/2024/07/03/2024-14414/anti-money-laundering-and-countering-the-financing-of-terrorism-programs). They also include banking clients and their personal financial connections, as well as broader interests such as national security and the global community.

## Ethical Risk
Financial safeguarding is accomplished through traditional methods such as policy frameworks, rule-based filters, and explicit programming, relying on predefined criteria and manual processes. The [current system has significant disparate impacts](https://charityandsecurity.org/system/files/Center%20for%20Global%20Devel%20Unintended%20Consequences%20Nov%202015.pdf) that are likely to be exacerbated by AI implementation unless they are specifically addressed. In efforts to [de-risk their portfolio](https://globalcenter.org/resource/understanding-bank-de-risking-and-its-effects-on-financial-inclusion/), financial institutions have [terminated relationships](https://www.nytimes.com/2023/04/08/your-money/bank-account-suspicious-activity.html) with entire groups of people because such institutions perceive them to increase their business, compliance, or regulatory risk. Indeed, [FATF standards result in financial exclusion](https://www.sciencedirect.com/science/article/pii/S2949791423000404) of disadvantaged groups such as women, disabled people, ethnic minorities, immigrants, low-income, rural, and undocumented people. Federated machine learning, which does not allow for direct inspection of the dataset, makes it more difficult to ensure diverse representation. And even access to a diverse data set alone is not enough, as federated learning algorithms also have a tendency towards [intrinsic bias](https://openreview.net/forum?id=V7CYzdruWdm) when compared to centralized learning. To avoid replicating and reinforcing these disparities, AI systems will need to be developed specifically with goals of inclusion. 

Widespread CFT policies also [systematically exclude](https://international-review.icrc.org/articles/whose-risk-bank-de-risking-and-politics-of-interpretation-and-vulnerability-in-the-mena-916) under-resourced countries and people living in active conflict, post-conflict, and disaster zones. Perpetual Client Risk Assessment relies on extensive and consistent access to client data, which disadvantages people with incomplete or non-traditional financial profiles. Additionally, individuals in these situations often use financial services in ways that the FATF and the U.S. Treasury [identifies as risky](https://home.treasury.gov/news/press-releases/jy2080), such as using money transfer services, sending and requesting international payments, soliciting individuals for cash, and using cryptocurrency. While these techniques may be exploited by valid counterterrorism targets, they are often the [only avenues available](https://charityandsecurity.org/system/files/Center%20for%20Global%20Devel%20Unintended%20Consequences%20Nov%202015.pdf) to individuals in crisis. AI-driven financial tools risk exacerbating their critical vulnerability.  

## Mitigation Strategy
Achieving equitable access to financial services within the CFT framework will require targeted efforts at every stage of the AI/ML algorithms lifecycle. Developers and deployers will need to adopt policy founded in financial inclusion, ensuring that training data is representative of groups that are historically and systemically under-resourced, both [nationally](https://home.treasury.gov/news/press-releases/jy2692) and [globally](https://www.fatf-gafi.org/en/topics/financial-inclusion.html). Inclusion strategies must be adaptable to [various contexts and evolving conditions](https://documents1.worldbank.org/curated/en/780821468333561612/pdf/909540BRI0Box300Inclusion0Sept02014.pdf), ensuring a clear distinction between terrorist financing and civilians relying on the financial system for their daily needs. 

Financial inclusion policies must also inform an ethical approach to user profiles with missing or incomplete data. Instead of flagging or denying service to profiles identified through pCRA or pKYC, an inclusion-first methodology examines the context behind missing data and explores innovative ways to include individuals, such as using [biometric identification](https://www.biometricupdate.com/202412/india-transforming-public-finance-with-digital-identity-and-biometrics) or [blockchain solutions](https://mintblue.com/blockchain-for-financial-inclusion/). Additionally, technical methods from other sectors, such as [Multiplicative Weight update with Regularization](https://arxiv.org/html/2309.07085v2) (MWR)—a technique used in medical imaging—may be useful for algorithmic handling of noisy or incomplete data. MWR not only optimizes the performance of the best-performing groups but also improves the performance of the worst-performing groups, effectively reducing group bias in fraud and CFT detection.
As federated learning is integrated into CFT algorithms, it will be critical to employ a fair or fairness-aware algorithm that [emphasizes fairness](https://www.mdpi.com/2079-9292/13/23/4664#) and [reduces bias](https://dl.acm.org/doi/10.1145/3631455). [Personalized federated models](https://www.sciencedirect.com/science/article/abs/pii/S0957417423033766) may be needed to allow for adaptations to regional and contextual variations. The algorithm should be supported by robust, evolving data schemes, particularly for detecting and mitigating bias in decentralized datasets. These schemes should be thoughtfully designed and tested to incorporate inclusion controls tailored to specific populations. During training and post-deployment, a [preference-aware monitoring scheme](https://arxiv.org/pdf/2404.08973) should be implemented to track individual model updates and evaluate [fairness in how they aggregate](https://arxiv.org/pdf/2405.16585), ensuring the model remains aligned with evolving sociocultural contexts and that potential biases are detected and mitigated throughout the federated learning process. Additionally, the CFT algorithm should maintain a high degree of explainability, while human operators require ongoing training in CFT, financial inclusion, and evolving sociocultural dynamics to ensure informed oversight.

## Final Thoughts
Although some advocates of advanced AI have touted it as a solution to many of the [financial sector’s challenges](https://internationalbanker.com/finance/is-ai-the-cornerstone-for-financial-inclusion/), uncritical AI adoption risks amplifying disparate impacts as long as the maximization of profits is the sole goal of the algorithm. To ensure that AI in counter-terrorist financing (CFT) supports both security and financial inclusion, we must adopt policies that prioritize fairness, transparency, and adaptability. Financial institutions should implement ethical AI frameworks that promote inclusivity, leveraging alternative data sources and alternative risk models to serve marginalized groups. Federated learning techniques, combined with robust monitoring and fairness-aware updates, can be used to address bias while maintaining privacy. Additionally, financial institutions must be transparent about AI decision-making, with independent audits to ensure equitable outcomes. By aligning financial inclusion with global standards and tailoring CFT strategies to specific regional needs, we can safeguard both national security and equitable access to financial services for all, particularly the most vulnerable populations.


